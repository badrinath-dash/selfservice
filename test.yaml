Description: >
  Deploys an EC2 cluster of 1 server running Splunk in the specified the provided VPC and subnets
  using an Auto Scaling Group
Metadata:
  LICENSE: Copyright Leading Directions pty ltd. 
  AWS::CloudFormation::Interface:
    ParameterGroups:
    - Label:
        default: Network Settings
      Parameters:
      - VPCId
      - LoadBalacerSubnets
      - AppSubnets
    - Label:
        default: Security Settings
      Parameters:
      - LoadBalancerProtocol
      - LoadBalancerCertArn
    - Label:
        default: Cluster Settings
      Parameters:
      - Environment
      - MinimumSize
      - MaximumSize
      - DesiredSize
      - AMI
      - InstanceType
      - RootVolumeSize
      - DataVolumeSize
    - Label:
        default: Splunk Settings
      Parameters:
      - SplunkAdminPassword
    - Label:
        default: Administration Settings
      Parameters:
      - SSMCloudWatchLogConfigKey          
    - Label:
        default: Domain Settings
      Parameters:
      - HostedZoneName
      - HostName      

    ParameterLabels:
      VPCId:
        default:  The VPC network that you want use for the resource.
      AppSubnets:
        default:  > 
            The network subnets that will be used for the application servers.
            Typically these are private subnets, as in most cases you do not require them to be directly internet accessible.
      LoadBalacerSubnets:
        default:  > 
            The network subnets that will be used for the application load balancers.
            Typically these are public subnets, as in most cases they are required to be directly internet accessible.
      LoadBalancerProtocol:
        default:  > 
            The network protocol to use for the load balancers (HTTP/HTTPS)
            Usually you will select HTTPS if you have a TLS certificate ARN, otherwise select HTTP.
      LoadBalancerCertArn:
        default: The ARN of the TLS (SSL/HTTPS) certificate that is registered an Amazon Certificate Manager.
      MinimumSize:
        default: > 
          The minimum number of servers that will be in the cluster.
          For high availability, choose a number greater than 1.
      MaximumSize:
        default: > 
          The maximum number of servers that will be in the cluster.
          This will be the maximum number of servers that can be run to meet autoscaling loads.
      DesiredSize:
        default: > 
          The desired number of servers that will be in the cluster.
          This number must be within the Min and Max Sizes.
      InstanceType:
        default: The instance type for each of the cluster servers.
      AMI:
        default: >
          This is the base image that the cluster will be built from.  
          In most cases, this in the Amazon Linux AMI.         
      SSMCloudWatchLogConfigKey:
        default: The existing System Manager Parameter Store key for cloudwatch log agent configuration.  
      HostedZoneName:
        default:  >
          The external DNS hosted zone name that is used to create additional recordsets within.
          This will typlically be performancereports.com.au
      HostName:
        default:  >
          The domain that will be registered to the load balancer DNS endpoint.
          Note - This MUST be a subdomain of the hosted zone name.
          This will typically be splunk.performancereports.com.au
      SplunkAdminPassword:
        default:  The admin password to set for the Splunk instance.   
      Environment:
        default: > 
          The environment that best describes this cluster.

Parameters:
  VPCId: 
    Type : AWS::EC2::VPC::Id
    Description : "VpcId of your existing Virtual Private Cloud (VPC)"
    ConstraintDescription : "must be the VPC Id of an existing Virtual Private Cloud."
  LoadBalacerSubnets:
    Type : List<AWS::EC2::Subnet::Id>
    Description : "One or more subnetIds in your Virtual Private Cloud (VPC) for the load balancer - typically these are public subnets."
    ConstraintDescription : "must be one or more existing subnets."
  AppSubnets:
    Type : List<AWS::EC2::Subnet::Id>
    Description : "One or more subnetIds in your Virtual Private Cloud (VPC) for the application - typically these are private subnets."
    ConstraintDescription : "must be one or more existing subnets."
  LoadBalancerProtocol:
    Description: Which protocol should the load balancer be listening on?
    Type: String
    Default: HTTPS
    AllowedValues:
      - HTTP
      - HTTPS
  LoadBalancerCertArn:
    Description: ACM registered certificate arn if HTTPS protocol is selected.
    Type: String
    Default: arn:aws:acm:ap-southeast-2:251200300274:certificate/b1606083-ef3a-4ea5-a08d-387b2011757b
  InstanceType:
    Description: Which instance type should we use to build the cluster?
    Type: String
    AllowedValues:
      - t3.micro
      - t3.small
      - t3.medium
      - t3.large
      - t3.xlarge
      - t3.2xlarge
      - m4.large
      - m4.xlarge
      - m4.2xlarge
      - m4.4xlarge
      - c5.large
      - c5.xlarge
      - c5.2xlarge
      - c5.4xlarge
    Default: c5.xlarge
  MinimumSize:
      Description: "Minimum number of hosts to create (should be equal to number of availablity zones)"
      Type: Number
      Default: 1
  MaximumSize:
      Description: "Maximum number of hosts to create (should be equal to number of availablity zones)"
      Type: Number
      Default: 1
  DesiredSize:
      Description: "Desired number of hosts to create (should be equal to number of availablity zones)"
      Type: Number
      Default: 1
  AMI:
    Description: The Application AMI ID for the instances in the cluster
    Type: AWS::EC2::Image::Id
    Default: ami-0d2fb06f3c1484132

  SSMCloudWatchLogConfigKey :
    Description: Name of parameter store which contains the json configuration of CWAgent.
    Type : String
    Default: /cloudwatch-log-agent-config-splunk
  HostedZoneName:
      Description: "The name of the registered Route53 host zone name.  eg performancereports.com.au"
      Type: String
      Default: "performancereports.com.au"
  HostName:
      Description: "The url of the solution  eg splunk.performancereports.com.au"
      Type: String
      Default: "splunkuat.performancereports.com.au"
  RootVolumeSize:
      Description: Size of the root filesystem block device in GB
      Type: Number
      Default: 50
  DataVolumeSize:
      Description: Size of the data filesystem block device in GB (must be greater or equal to the snapshot if specified)
      Type: Number
      Default: 500
  SplunkAdminPassword:
      Description: The admin password for the splunk instance.
      Type: String
      NoEcho: true
  Environment:
    Type: String
    Default: production    
    Description: "The environment type for this solution."
    AllowedValues:
      - production
      - uat
  S3DataBucket:
    Description: The name of the bucket to persist Splunk data and configurations
    Type: String
    Default: leadingdirections-data-splunk-uat

Conditions:
  EnableBasicLoadBalancer: !Equals [!Ref LoadBalancerProtocol, "HTTP"]
  EnableSecureLoadBalancer: !Equals [!Ref LoadBalancerProtocol, "HTTPS"]
  EnableHighAvailability: !Equals [!Ref Environment, "production"]

Resources:
          
  LoadBalancer:
    Type: AWS::ElasticLoadBalancingV2::LoadBalancer
    Properties:
      LoadBalancerAttributes:
      - Key: 'routing.http2.enabled'
        Value: 'false'
      Subnets: !Ref LoadBalacerSubnets
      SecurityGroups: 
          - !Ref LoadBalancerSecurityGroup
      Tags:
        - Key: Name
          Value: !Join [ "-", [ Ref: "AWS::StackName", "alb" ] ]

  BasicLoadBalancerListener:
    Type: AWS::ElasticLoadBalancingV2::Listener
    Properties:
      LoadBalancerArn: !Ref LoadBalancer
      Port: 80
      Protocol: HTTP
      DefaultActions:
        - !If
          - EnableSecureLoadBalancer
          - RedirectConfig:
              Host: "#{host}"
              Path: "/#{path}"
              Port: "443"
              Protocol: "HTTPS"
              Query: "#{query}"
              StatusCode: HTTP_301
            Type: "redirect"
          - Type: forward
            TargetGroupArn: !Ref TargetGroup



  SecureLoadBalancerListener:
    Type: AWS::ElasticLoadBalancingV2::Listener
    Condition: EnableSecureLoadBalancer
    Properties:
      LoadBalancerArn: !Ref LoadBalancer
      Certificates:
        - CertificateArn: !Ref LoadBalancerCertArn
      Port: 443
      Protocol: !Ref LoadBalancerProtocol
      DefaultActions:
        - Type: forward
          TargetGroupArn: !Ref TargetGroup
      SslPolicy: ELBSecurityPolicy-TLS13-1-2-2021-06

  SecureLoadBalancerListenerPort8089:
    Type: AWS::ElasticLoadBalancingV2::Listener
    Condition: EnableSecureLoadBalancer
    Properties:
      LoadBalancerArn: !Ref LoadBalancer
      Certificates:
        - CertificateArn: !Ref LoadBalancerCertArn
      Port: 8089
      Protocol: !Ref LoadBalancerProtocol
      DefaultActions:
        - Type: forward
          TargetGroupArn: !Ref TargetGroupPort8089
      SslPolicy: ELBSecurityPolicy-TLS13-1-2-2021-06

  TargetGroup:
    Type: AWS::ElasticLoadBalancingV2::TargetGroup
    Properties:
      VpcId: !Ref VPCId
      Port: 8000
      Protocol: HTTP
      HealthCheckPort: '8000'
      HealthCheckPath: '/en-GB/account/login'
      HealthCheckProtocol: HTTP
      HealthCheckTimeoutSeconds: 10
      HealthyThresholdCount: 3
      Matcher:
        HttpCode: 200,303
      Tags:
        - Key: Name
          Value: !Join [ "-", [ Ref: "AWS::StackName", "tg" ] ]

  TargetGroupPort8089:
    Type: AWS::ElasticLoadBalancingV2::TargetGroup
    Properties:
      VpcId: !Ref VPCId
      Port: 8089
      Protocol: HTTP
      HealthCheckPort: '8000'
      HealthCheckPath: '/en-GB/account/login'
      HealthCheckProtocol: HTTP
      HealthCheckTimeoutSeconds: 10
      HealthyThresholdCount: 3
      Matcher:
        HttpCode: 200,303
      Tags:
        - Key: Name
          Value: !Join [ "-", [ Ref: "AWS::StackName", "tg" ] ]

  AutoScalingGroup:
    Type: AWS::AutoScaling::AutoScalingGroup
    Properties:
      VPCZoneIdentifier: !Ref AppSubnets
      LaunchConfigurationName: !Ref LaunchConfiguration
      MinSize: !Ref MinimumSize
      MaxSize: !Ref MaximumSize
      DesiredCapacity: !Ref DesiredSize
      HealthCheckType: !If [EnableHighAvailability, 'EC2', 'EC2']
      HealthCheckGracePeriod: 300
      TargetGroupARNs:
          - !Ref TargetGroup
          - !Ref TargetGroupPort8089
      Tags:
        - Key: Name
          Value: !Join [ "-", [ Ref: "AWS::StackName", "asg" ] ]
          PropagateAtLaunch: true
        - Key: role
          Value: splunk
          PropagateAtLaunch: true
        - Key: environment
          Value: !Ref Environment
          PropagateAtLaunch: true
    CreationPolicy:
      ResourceSignal:
        Timeout: PT60M
    UpdatePolicy:
      AutoScalingRollingUpdate:
        MinInstancesInService: 0
        MaxBatchSize: 1
        PauseTime: PT60M
        SuspendProcesses:
          - HealthCheck
          - ReplaceUnhealthy
          - AZRebalance
          - AlarmNotification
          - ScheduledActions
        WaitOnResourceSignals: true

  TargetTrackingPolicy:
    Type: AWS::AutoScaling::ScalingPolicy
    Properties:
      AutoScalingGroupName:
        Ref: AutoScalingGroup
      PolicyType: TargetTrackingScaling
      TargetTrackingConfiguration:
        PredefinedMetricSpecification:
          PredefinedMetricType: ASGAverageCPUUtilization
        TargetValue: 60

  LaunchConfiguration:
    Type: AWS::AutoScaling::LaunchConfiguration
    CreationPolicy:
        ResourceSignal:
          Timeout: PT10M
          Count:
             Ref: DesiredSize
    Properties:
      ImageId: !Ref AMI
      InstanceType: !Ref InstanceType
      BlockDeviceMappings:
          - DeviceName: "/dev/xvda"
            Ebs:
              VolumeSize: !Ref RootVolumeSize
              VolumeType: gp2
              DeleteOnTermination: true
              Encrypted: true
      SecurityGroups:
        - !Ref InstanceSecurityGroup
      IamInstanceProfile: !Ref InstanceProfile
      UserData:
        Fn::Base64: !Sub |
          #!/bin/bash -v

          #Set the default Timezone to UTC.
          timedatectl set-timezone UTC

          #Set the environment paths
          echo 'export PATH=/usr/local/bin:/usr/bin:$PATH' >> /etc/environment
          echo 'export SPLUNK_HOME=/opt/splunk' >> /etc/environment
          . /etc/environment
          
          #Install Python3
          yum -y install python3
          curl -O https://bootstrap.pypa.io/get-pip.py
          python3 get-pip.py
          pip3 install --upgrade pip 

          #Install AWSCLI
          yum remove awscli -y
          curl "https://awscli.amazonaws.com/awscli-exe-linux-x86_64.zip" -o "awscliv2.zip"
          unzip awscliv2.zip
          ./aws/install
          pip3 install boto3
          
          #Install AWS SSM agent
          yum install -y https://s3.amazonaws.com/ec2-downloads-windows/SSMAgent/latest/linux_amd64/amazon-ssm-agent.rpm
          systemctl enable amazon-ssm-agent
          systemctl start amazon-ssm-agent

          #Install AWS Cloudwatch agent
          yum install -y  https://s3.amazonaws.com/amazoncloudwatch-agent/centos/amd64/latest/amazon-cloudwatch-agent.rpm 
          /opt/aws/amazon-cloudwatch-agent/bin/amazon-cloudwatch-agent-ctl -a fetch-config -m ec2 -c ssm:${SSMCloudWatchLogConfigKey} -s
          systemctl enable amazon-cloudwatch-agent
          systemctl restart amazon-cloudwatch-agent

          #Install AWS CodeDeploy agent
          yum install -y ruby
          yum install -y https://s3.amazonaws.com/aws-codedeploy-us-east-1/latest/codedeploy-agent.noarch.rpm
          systemctl enable codedeploy-agent
          systemctl restart codedeploy-agent

          # Search for an un-associated EBS volume with a tag of purpose: ${AWS::StackName}-data to associate with this instance and mount to /opt/splunk/data
          #   find the ebs volume that is available
          export VOLUME_ID=$(aws ec2 describe-volumes --filters Name=status,Values=available Name=tag:purpose,Values=${AWS::StackName}-data --query "sort_by(Volumes, &CreateTime)[-1].{ID:VolumeId}" --region ${AWS::Region} --output text)
          export AVAILABILITY_ZONE=$(curl http://169.254.169.254/latest/meta-data/placement/availability-zone)
          export INSTANCE_ID=$(curl http://169.254.169.254/latest/meta-data/instance-id)


          if [[ -n "$VOLUME_ID" && "$VOLUME_ID" != "None" ]]; then
            echo "Found a valid volume $VOLUME_ID, checking the availability zone."
            export VOLUME_AZ=$(aws ec2 describe-volumes --volume-ids $VOLUME_ID --filters Name=status,Values=available Name=tag:purpose,Values=${AWS::StackName}-data --query "Volumes[*].{AZ:AvailabilityZone}" --region ${AWS::Region} --output text)
            if [[ "$VOLUME_AZ" != "AVAILABILITY_ZONE" ]]; then
              echo "The Volume AZ does not match the instance AZ, creating a snapshot, then volume in the correct AZ"
              aws ec2 create-snapshot --volume-id $VOLUME_ID --description '${AWS::StackName}-data-AddHocSnapshot' --tag-specifications 'ResourceType=snapshot,Tags=[{Key=purpose,Value=${AWS::StackName}-data-AddHocSnapshot},{Key=Name,Value=${AWS::StackName}-data-AddHocSnapshot}]' --region ${AWS::Region}
              # wait for a while for the snapshot to be completed.
              secs=300                         # Set interval (duration) in seconds.
              endTime=$(( $(date +%s) + secs )) # Calculate end time.

              while [ $(date +%s) -lt $endTime ]; do  # Loop until interval has elapsed.
                export ADDHOC_SNAPSHOT_ID=$(aws ec2 describe-snapshots --filters Name=status,Values=completed Name=tag:purpose,Values=${AWS::StackName}-data-AddHocSnapshot --query "sort_by(Snapshots, &StartTime)[-1].{ID:SnapshotId}" --region ${AWS::Region} --output text)
                if [[ -n "$ADDHOC_SNAPSHOT_ID" && "$ADDHOC_SNAPSHOT_ID" != "None" ]]; then
                  echo "Found the created snapshot, to create a volume from of size ${DataVolumeSize}GB, SnapshotId: $ADDHOC_SNAPSHOT_ID"
                  aws ec2 create-volume --volume-type gp2 --size ${DataVolumeSize} --snapshot-id $ADDHOC_SNAPSHOT_ID  --availability-zone $AVAILABILITY_ZONE --tag-specifications 'ResourceType=volume,Tags=[{Key=purpose,Value=${AWS::StackName}-data},{Key=Name,Value=${AWS::StackName}-data}]'
                  break
                fi
              done

            fi
          else
            echo "No volume was found, so checking for a ebs snapshot"
          
            # find the most recent ebs snapshot with a tag of purpose: ${AWS::StackName}-data, use it to create an ebs volume, associate it and mount
            export SNAPSHOT_ID=$(aws ec2 describe-snapshots --filters Name=status,Values=completed Name=tag:purpose,Values=${AWS::StackName}-data --query "sort_by(Snapshots, &StartTime)[-1].{ID:SnapshotId}" --region ${AWS::Region} --output text)
            if [[ -n "$SNAPSHOT_ID" && "$SNAPSHOT_ID" != "None" ]]; then
              # Create a volume from the snapshot at the specified size
              echo "Found a valid snapshot to create a volume from of size ${DataVolumeSize}GB, SnapshotId: $SNAPSHOT_ID"
              aws ec2 create-volume --volume-type gp2 --size ${DataVolumeSize} --snapshot-id $SNAPSHOT_ID  --availability-zone $AVAILABILITY_ZONE --tag-specifications 'ResourceType=volume,Tags=[{Key=purpose,Value=${AWS::StackName}-data},{Key=Name,Value=${AWS::StackName}-data}]'
            else
              echo "Unable to find a suitable EBS volume or snapshot, creating an empty volume."
              aws ec2 create-volume --volume-type gp2 --size ${DataVolumeSize} --availability-zone $AVAILABILITY_ZONE --tag-specifications 'ResourceType=volume,Tags=[{Key=purpose,Value=${AWS::StackName}-data},{Key=Name,Value=${AWS::StackName}-data}]'
              export NEW_VOLUME=1
            fi
          fi

          # wait for a while for the volume to be created.
          secs=300                         # Set interval (duration) in seconds.
          endTime=$(( $(date +%s) + secs )) # Calculate end time.

          while [ $(date +%s) -lt $endTime ]; do  # Loop until interval has elapsed.
            export VOLUME_ID=$(aws ec2 describe-volumes --filters Name=availability-zone,Values=$AVAILABILITY_ZONE Name=status,Values=available Name=tag:purpose,Values=${AWS::StackName}-data --query "sort_by(Volumes, &CreateTime)[-1].{ID:VolumeId}" --region ${AWS::Region} --output text)
            if [[ -n "$VOLUME_ID" && "$VOLUME_ID" != "None" ]]; then
              echo "Found a suitable volume, and it is now available for use. VolumeId: $VOLUME_ID"
              break
            fi
          done

          # clean up any add-hoc snapshots
          if [ -n "$ADDHOC_SNAPSHOT_ID" ]; then 
            echo "Deleting add-hoc snapshot that was used to migrate the EBS volume to another AZ."
            aws ec2 delete-snapshot --snapshot-id $ADDHOC_SNAPSHOT_ID --region ${AWS::Region}
          fi

          #   associate it to this instance
          echo "Attaching the volume to the instance.  InstanceId: $INSTANCE_ID  VolumeId: $VOLUME_ID"
          aws ec2 attach-volume --volume-id $VOLUME_ID --instance-id $INSTANCE_ID --device /dev/xvdf --region ${AWS::Region}

          # wait for a while for the volume to be attached.
          secs=300                         # Set interval (duration) in seconds.
          endTime=$(( $(date +%s) + secs )) # Calculate end time.

          while [ $(date +%s) -lt $endTime ]; do  # Loop until interval has elapsed.
            export ATTACHED_VOLUME_ID=$(aws ec2 describe-volumes --volume-ids $VOLUME_ID --filters Name=attachment.status,Values=attached --query "Volumes[*].{ID:VolumeId}" --region ${AWS::Region} --output text)
            if [[ -n "$ATTACHED_VOLUME_ID" && "$ATTACHED_VOLUME_ID" != "None" ]]; then
              echo "Volume is attached, and it is now available for use. VolumeId: $VOLUME_ID"
              break
            fi
          done

          #   mount it 
          echo "Resizing and Mounting the EBS volume onto the instance."

          if [ -n "$NEW_VOLUME" ]; then 
            echo "Formatting the new EBS volume"
            /sbin/pvcreate /dev/xvdf
            /sbin/vgcreate vg0 /dev/xvdf
            /sbin/lvcreate -l 100%FREE -n myapp vg0
            mkfs.ext4 /dev/vg0/myapp
          else
            echo "Resizing the new EBS volume, if required."
            /sbin/pvresize /dev/xvdf
            /sbin/lvextend -l +100%FREE -n /dev/mapper/vg0-myapp
            /sbin/e2fsck -fy /dev/mapper/vg0-myapp
            /sbin/resize2fs /dev/mapper/vg0-myapp
          fi

          echo "Mounting the EBS volume."
          mkdir -p /opt/splunk/data
          echo "/dev/mapper/vg0-myapp /opt/splunk/data ext4 defaults 0 2" >> /etc/fstab
          mount -a
          touch /opt/splunk/data/file1.txt
          rm -f /opt/splunk/data/file1.txt

          #Install SPLUNK following instructions.
          wget -O /opt/splunk-8.0.1-6db836e2fb9e-Linux-x86_64.tgz https://download.splunk.com/products/splunk/releases/8.0.1/linux/splunk-8.0.1-6db836e2fb9e-Linux-x86_64.tgz
          cd /opt && tar zxf splunk-8.0.1-6db836e2fb9e-Linux-x86_64.tgz && rm -f splunk-8.0.1-6db836e2fb9e-Linux-x86_64.tgz
          $SPLUNK_HOME/bin/splunk stop

          #Set admin password and accept license agreement
          $SPLUNK_HOME/bin/splunk start --accept-license --answer-yes --no-prompt --seed-passwd ${SplunkAdminPassword}

          #enable boot-start
          $SPLUNK_HOME/bin/splunk stop
          $SPLUNK_HOME/bin/splunk enable boot-start -systemd-managed 1 -user root

          # Configure IP and hostname settings
          export LOCALIP=$(curl http://169.254.169.254/latest/meta-data/local-ipv4)
          export INSTANCEID=$(curl http://169.254.169.254/latest/meta-data/instance-id)
          printf '%s\t%s\n' \"$LOCALIP\" 'splunk' >> /etc/hosts
          hostname splunk

          # Configure Splunk data sync
          mkdir -p $SPLUNK_HOME/scripts

          #Install aws cfn bootstrap helpers
          yum update -y aws-cfn-bootstrap

          /opt/aws/bin/cfn-init -v  --stack ${AWS::StackName} --resource LaunchConfiguration --region ${AWS::Region}

          /opt/aws/bin/cfn-signal -e $? --stack ${AWS::StackName} --resource AutoScalingGroup --region ${AWS::Region}       

    Metadata:
      AWS::CloudFormation::Init:
        config:
          commands:       
            050-s3-splunk-initialise:
              command:
                !Sub |
                  /opt/splunk/scripts/s3splunkinitialise.sh
          files:
            /opt/splunk/scripts/s3splunksync.sh:
              mode: "000755"
              owner: root
              group: root
              content: !Sub |
                #!/bin/bash
                echo "Starting S3 Splunk Sync"
                aws s3 sync /opt/splunk/etc/licenses s3://${S3DataBucket}/etc/licenses --only-show-errors
                aws s3 cp /opt/splunk/etc/apps/search/local/indexes.conf s3://${S3DataBucket}/etc/apps/search/local/indexes.conf --only-show-errors
                aws s3 cp /opt/splunk/etc/apps/search/local/savedsearches.conf s3://${S3DataBucket}/etc/apps/search/local/savedsearches.conf --only-show-errors
                aws s3 cp /opt/splunk/etc/apps/search/local/inputs.conf s3://${S3DataBucket}/etc/apps/search/local/inputs.conf --only-show-errors
                aws s3 cp /opt/splunk/etc/system/local/props.conf s3://${S3DataBucket}/etc/system/local/props.conf --only-show-errors
                aws s3 cp /opt/splunk/etc/system/local/limits.conf s3://${S3DataBucket}/etc/system/local/limits.conf --only-show-errors
                ## Copy custom roles to S3 Bucket
                aws s3 cp /opt/splunk/etc/system/local/authorize.conf s3://${S3DataBucket}/etc/system/local/authorize.conf  --only-show-errors
                ## Copy Splunk users to S3 Bucket 
                aws s3 cp /opt/splunk/etc/passwd s3://${S3DataBucket}/etc/passwd  --only-show-errors
                ## Copy splunk user settings to S3 Bucket
                aws s3 cp --recursive /opt/splunk/etc/users s3://${S3DataBucket}/etc/users  --only-show-errors
                echo "Finished S3 Splunk Sync"

            /opt/splunk/scripts/syncs3csvtosplunk.py:
              mode: "000755"
              owner: root
              group: root
              content: !Sub |
                #!/usr/bin/env python3
                import boto3, datetime, os
                import pathlib

                splunk_folder='/opt/splunk/data'
                bucket='${S3DataBucket}'
                prefix='csv'
                cutoff_days=3
                delete_days=cutoff_days+2

                s3 = boto3.client('s3')
                paginator = s3.get_paginator('list_objects_v2')
                pages = paginator.paginate(Bucket=bucket, Prefix=prefix)

                #Delete old CSV files from splunk folder.
                print(f'Deleting {prefix} files older than {delete_days} days')
                os.system("find " + f"{splunk_folder}/{prefix}" + f" -mtime +{delete_days} -print")
                os.system("find " + f"{splunk_folder}/{prefix}" + f" -mtime +{delete_days} -delete")

                #Copy recent CSV files from S3
                for page in pages:
                    for obj in page['Contents']:
                        if (datetime.datetime.utcnow().replace(tzinfo=datetime.timezone.utc) - obj['LastModified']).days <= cutoff_days:
                          filename = f"{splunk_folder}/{obj['Key']}"
                          if not os.path.exists(filename):
                            print(f"Copying s3 object {obj['Key']} to {filename}")
                            # Create the nested parent folders if they don't exist
                            p = pathlib.Path(filename)
                            pathlib.Path(p.parents[0]).mkdir(parents=True, exist_ok=True)
                            s3.download_file(bucket, obj['Key'], filename)
                print('Finished')

            /opt/splunk/scripts/s3splunkinitialise.sh:
              mode: "000755"
              owner: root
              group: root
              content: !Sub |
                #!/bin/bash
                echo "Starting S3 Splunk Initialisation"
                aws s3 sync s3://${S3DataBucket}/etc/licenses /opt/splunk/etc/licenses --only-show-errors --delete
                aws s3 cp s3://${S3DataBucket}/etc/apps/search/local/indexes.conf /opt/splunk/etc/apps/search/local/indexes.conf --only-show-errors
                aws s3 cp s3://${S3DataBucket}/etc/apps/search/local/inputs.conf /opt/splunk/etc/apps/search/local/inputs.conf --only-show-errors
                aws s3 cp s3://${S3DataBucket}/etc/apps/search/local/savedsearches.conf /opt/splunk/etc/apps/search/local/savedsearches.conf --only-show-errors
                aws s3 cp s3://${S3DataBucket}/etc/system/local/props.conf /opt/splunk/etc/system/local/props.conf  --only-show-errors
                aws s3 cp s3://${S3DataBucket}/etc/system/local/limits.conf /opt/splunk/etc/system/local/limits.conf  --only-show-errors
                ## Restore custom roles from S3 Bucket
                aws s3 cp s3://${S3DataBucket}/etc/system/local/authorize.conf /opt/splunk/etc/system/local/authorize.conf --only-show-errors
                ## Restore custom users from S3 Bucket
                aws s3 cp s3://${S3DataBucket}/etc/passwd /opt/splunk/etc/passwd --only-show-errors
                ## Restore custom user configuration from S3 Bucket
                aws s3 cp --recursive s3://${S3DataBucket}/etc/users /opt/splunk/etc/users --only-show-errors
                echo '0 0 * * * root /opt/splunk/scripts/s3splunksync.sh' > /etc/cron.d/s3splunksync
                echo "Finished S3 Splunk Initialisation"
                echo "Configuring and restarting Splunk service."
                sed -i 's|.*SPLUNK_DB=.*|SPLUNK_DB=/opt/splunk/data/db|' /opt/splunk/etc/splunk-launch.conf
                sed -i 's|.*serverName = .*|serverName = ${HostName}|' /opt/splunk/etc/system/local/server.conf
                sed -i 's|.*enableSplunkdSSL = .*|enableSplunkdSSL = false|' /opt/splunk/etc/system/local/server.conf
                sed -i 's|.*enableSplunkdSSL = .*|enableSplunkdSSL = false|' /opt/splunk/etc/system/default/server.conf
                sed -i 's|.*host = .*|host = ${HostName}|' /opt/splunk/etc/system/local/inputs.conf
                grep -q '\[license\]' /opt/splunk/etc/system/local/server.conf || echo "[license]" >> /opt/splunk/etc/system/local/server.conf
                grep -q 'active_group =' /opt/splunk/etc/system/local/server.conf || echo "active_group = Enterprise" >> /opt/splunk/etc/system/local/server.conf

                systemctl restart Splunkd
                echo "Splunk service restarted."
            /etc/cfn/cfn-hup.conf:
              mode: 000400
              owner: root
              group: root
              content: !Sub |
                [main]
                stack=${AWS::StackId}
                region=${AWS::Region}

            /etc/cfn/hooks.d/cfn-auto-reloader.conf:
              content: !Sub |
                [cfn-auto-reloader-hook]
                triggers=post.update
                path=Resources.LaunchConfiguration.Metadata.AWS::CloudFormation::Init
                action=/opt/aws/bin/cfn-init -v --region ${AWS::Region} --stack ${AWS::StackName} --resource LaunchConfiguration
          services:
            sysvinit:
              cfn-hup:
                enabled: true
                ensureRunning: true
                files:
                  - /etc/cfn/cfn-hup.conf
                  - /etc/cfn/hooks.d/cfn-auto-reloader.conf


  SplunkCSVDocument:
    Type: AWS::SSM::Document
    Properties:
      DocumentType: Command
      Content:
        schemaVersion: "2.2"
        description: "Load CSV files from S3 into Splunk."
        mainSteps:
        - action: "aws:runShellScript"
          name: "SyncS3CSVtoSplunk"
          inputs:
            timeoutSeconds: 3600
            isEnd: true
            runCommand:
            - "echo 'Downloading recent CSV files from S3 to Splunk.'"
            - "sudo su"
            - "python3 /opt/splunk/scripts/syncs3csvtosplunk.py"

  DBUpdateDocumentAssociation:
    Type: AWS::SSM::Association
    Properties: 
      AssociationName: !Sub 'SplunkCSVDocument-${AWS::StackName}' 
      Name: !Ref SplunkCSVDocument
      ScheduleExpression: "cron(0 0/30 * 1/1 * ? *)"
      Targets: 
        - Key: tag:aws:autoscaling:groupName
          Values: 
            - !Ref AutoScalingGroup

  InstanceRole:
    Type: AWS::IAM::Role
    Properties:
      Path: /
      AssumeRolePolicyDocument:
        Statement:
          - Action:
              - 'sts:AssumeRole'
            Principal:
              Service:
                - ec2.amazonaws.com
            Effect: Allow
      Policies:
      - PolicyName:
          Fn::Join:
          - ''
          - - Ref: AWS::StackName
            - .InstanceRole
        PolicyDocument:
          Statement:              
          - Effect: Allow
            Action:
              - cloudwatch:PutMetricData
              - cloudwatch:GetMetricStatistics
              - cloudwatch:DescribeAlarms
              - cloudwatch:PutMetricAlarm
            Resource: "*"
          - Effect: Allow
            Action:
              - logs:CreateLogGroup
              - logs:CreateLogStream
              - logs:PutLogEvents
              - logs:DescribeLogStreams
            Resource: "arn:aws:logs:*:*:*"
          - Effect: Allow
            Action:
              - ssm:GetParameter
            Resource: "*"
          - Effect: Allow
            Action:
              - ec2:*
            Resource: "*"
          - Effect: Allow
            Action:
              - cloudformation:Describe*
              - cloudformation:Signal*
            Resource: "*"
      ManagedPolicyArns:
        - arn:aws:iam::aws:policy/service-role/AmazonEC2RoleforSSM
        - arn:aws:iam::aws:policy/CloudWatchAgentServerPolicy
        - arn:aws:iam::aws:policy/service-role/AWSCodeDeployRole

  InstanceProfile:
    Type: AWS::IAM::InstanceProfile
    Properties:
      Path: /
      Roles:
        - !Ref InstanceRole

  InstanceSecurityGroup:
    Type: AWS::EC2::SecurityGroup
    Properties:
      GroupDescription: !Sub '${AWS::StackName} Instance security group'
      VpcId: !Ref VPCId
      SecurityGroupIngress:
        - IpProtocol: tcp
          FromPort: 8000
          ToPort: 8000
          CidrIp: 0.0.0.0/0
        - IpProtocol: tcp
          FromPort: 8089
          ToPort: 8089
          CidrIp: 0.0.0.0/0
        - IpProtocol: tcp
          FromPort: 8065
          ToPort: 8065
          CidrIp: 0.0.0.0/0
        - IpProtocol: tcp
          FromPort: 9997
          ToPort: 9997
          CidrIp: 0.0.0.0/0
        - IpProtocol: udp
          FromPort: 514
          ToPort: 514
          CidrIp: 0.0.0.0/0


  LoadBalancerSecurityGroup:
    Type: AWS::EC2::SecurityGroup
    Properties:
      GroupDescription: !Sub '${AWS::StackName} Load Balancer security group'
      VpcId: !Ref VPCId
      SecurityGroupIngress:
        - IpProtocol: tcp
          FromPort: 80
          ToPort: 80
          CidrIp: 0.0.0.0/0
        - IpProtocol: tcp
          FromPort: 443
          ToPort: 443
          CidrIp: 0.0.0.0/0
        - IpProtocol: tcp
          FromPort: 8089
          ToPort: 8089
          CidrIp: 0.0.0.0/0

  EBSDataVolumeLifecyclePolicy:
    Type: "AWS::DLM::LifecyclePolicy"
    Properties:
      Description: !Sub "${AWS::StackName}-data-DailySnapshot"
      State: "ENABLED"
      ExecutionRoleArn: !Sub arn:aws:iam::${AWS::AccountId}:role/service-role/AWSDataLifecycleManagerDefaultRole
      PolicyDetails:
        ResourceTypes:
          - "VOLUME"
        TargetTags:
          -
            Key: "purpose"
            Value: !Sub "${AWS::StackName}-data"
          
        Schedules:
          -
            Name: "Daily Snapshots"
            TagsToAdd:
              -
                Key: "type"
                Value: "DailySnapshot"
              -
                Key: "autoscaling-groupname"
                Value: !Ref AutoScalingGroup
              
            CreateRule:
              Interval: 12
              IntervalUnit: "HOURS"
              Times:
                - "02:00"
            RetainRule:
              Count: 14
            CopyTags: true


### DOMAIN NAME RESOURCES

  RecordSet:
    Type: AWS::Route53::RecordSet
    Properties:
      HostedZoneName: !Join ['', [!Ref 'HostedZoneName', .]]
      Comment: DNS name for the admin load balancer.
      Name: !Join ['', [!Ref 'HostName', .]]
      Type: A
      AliasTarget:
        HostedZoneId: !GetAtt LoadBalancer.CanonicalHostedZoneID
        DNSName: !GetAtt LoadBalancer.DNSName

Outputs:

  AutoScalingGroupName:
    Description: A reference to AutoScaling Group Name
    Value: !Ref AutoScalingGroup
    Export:
      Name: !Sub "${AWS::StackName}-AutoScalingGroupNameId"

  HostName:
    Description: The host name url
    Value: !Ref HostName
    Export:
      Name: !Sub "${AWS::StackName}-HostName"

  HostedZoneName:
    Description: The hosted zone name url
    Value: !Ref HostedZoneName
    Export:
      Name: !Sub "${AWS::StackName}-HostedZoneName"
